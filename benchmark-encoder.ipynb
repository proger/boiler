{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'll be testing model configurations to find out optimal batch_size/latency values for the encoder.\n",
    "\n",
    "I will be testing cuda/cpu + fp16/amp/fp32, but not int8 as FBGEMM does not support transposed convolutions (yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: GeForce GTX 1080 Ti (UUID: GPU-80ea3a86-9161-af28-825a-3e8fccb01b72)\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor\t: 0\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 158\r\n",
      "model name\t: Intel(R) Core(TM) i5-8400 CPU @ 2.80GHz\r\n",
      "stepping\t: 10\r\n",
      "microcode\t: 0xde\r\n",
      "cpu MHz\t\t: 3446.669\r\n",
      "cache size\t: 9216 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 6\r\n",
      "core id\t\t: 0\r\n",
      "cpu cores\t: 6\r\n",
      "apicid\t\t: 0\r\n",
      "initial apicid\t: 0\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 22\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:          32061       10692       19536           5        1832       20961\r\n",
      "Swap:          2047        2044           3\r\n"
     ]
    }
   ],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings(\"ignore\"); import os; os.environ['KALDI_ROOT'] = '/tmp' # ignore warnings\n",
    "\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import boiler.dataset\n",
    "import boiler.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    wav_dir = Path('/home/proger/coub-crawler/monthlyLog/wav')\n",
    "    pt_path = Path('/home/proger/boiler/exp/p_t64_b512/vqvae_223.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1).cuda() # warming up cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_stats(0)['active.all.allocated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 5632)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memstats = torch.cuda.memory_stats(0)\n",
    "memstats['allocated_bytes.all.current'], memstats['allocated_bytes.all.allocated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_batch = next(iter(DataLoader(ConcatDataset([boiler.dataset.WavFile(wav) for wav in sorted(args.wav_dir.glob('*.wav'))]),\n",
    "                                    batch_size=256, shuffle=True, num_workers=4)))\n",
    "\n",
    "cpu_model = boiler.encoder.BagTopVQVAE(args.pt_path, device='cpu')\n",
    "golden_embeddings = cpu_model.forward(golden_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = {\n",
    "    'cpu': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 24, 32, 64],\n",
    "    'cuda': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 24, 32, 64, 64+32, 128, 128+32, 128+64, 256, 512, 1024, 2048, 4096]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_type(model):\n",
    "    return next(model.parameters()).device.type\n",
    "\n",
    "def dtype(model):\n",
    "    return next(model.parameters()).dtype\n",
    "\n",
    "def size(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    lambda: cpu_model,\n",
    "    lambda: torch.jit.script(cpu_model),\n",
    "    #lambda: cpu_model.half(), # these are generally very lossy\n",
    "    #lambda: torch.jit.script(cpu_model.half()),\n",
    "    lambda: boiler.encoder.BagTopVQVAE(args.pt_path, device='cuda'),\n",
    "    lambda: torch.jit.script(boiler.encoder.BagTopVQVAE(args.pt_path, device='cuda')),\n",
    "    #lambda: boiler.encoder.BagTopVQVAE(args.pt_path, device='cuda').half(),\n",
    "    #lambda: torch.jit.script(boiler.encoder.BagTopVQVAE(args.pt_path, device='cuda').half()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:31<01:33, 31.03s/it]"
     ]
    }
   ],
   "source": [
    "for mkmodel in tqdm(models):\n",
    "    model = mkmodel()\n",
    "    \n",
    "    for autocast_enabled in (True, False):\n",
    "        with torch.cuda.amp.autocast(autocast_enabled):\n",
    "\n",
    "            for batch_size in batch_sizes[device_type(model)]:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                repeat_dim = max(batch_size//golden_batch.size(0),1),1,1\n",
    "                batch = golden_batch.repeat(*repeat_dim)[:batch_size]\n",
    "                batch = batch.to(device_type(model)).type(dtype(model))\n",
    "\n",
    "                loss_sample_batch_size = min(batch.size(0), golden_batch.size(0))\n",
    "\n",
    "                try:\n",
    "                    def run():\n",
    "                        a = time.perf_counter()\n",
    "                        results = model.forward(batch)\n",
    "                        b = time.perf_counter()\n",
    "\n",
    "                        loss = 1 - F.cosine_similarity(golden_embeddings[:loss_sample_batch_size],\n",
    "                                                       results[:loss_sample_batch_size].cpu())\n",
    "                        return b-a, loss.mean().item()\n",
    "\n",
    "                    stats = torch.tensor([run() for _ in range(10)])\n",
    "                    latency, loss = stats.mean(dim=0).tolist()\n",
    "                    latency_std, loss_std = stats.std(dim=0).tolist()\n",
    "\n",
    "                    exps.append(dict(batch_size=batch_size,\n",
    "                                     latency=latency, latency_std=latency_std,\n",
    "                                     loss=loss, loss_std=loss_std,\n",
    "                                     device=device_type(model), dtype=dtype(model),\n",
    "                                     type=type(model), size=size(model),\n",
    "                                     autocast=autocast_enabled,\n",
    "                                     reason='ok'))\n",
    "                except RuntimeError as e:\n",
    "                    exps.append(dict(batch_size=batch_size,\n",
    "                                     latency=float('NaN'), latency_std=float('NaN'),\n",
    "                                     loss=float('NaN'), loss_std=float('NaN'),\n",
    "                                     device=device_type(model), dtype=dtype(model),\n",
    "                                     type=type(model), size=size(model),\n",
    "                                     autocast=autocast_enabled,\n",
    "                                     reason=str(e)))\n",
    "                finally:\n",
    "                    del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(exps)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('benchmark-encoder.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffix(s):\n",
    "    return lambda x: s if x else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kind'] = df['device'] + '-' + df['dtype'].apply(repr) + df['type'].apply(repr).str.contains('jit').apply(suffix('-jit')) + df['autocast'].apply(suffix('-autocast')) + '-' + df['size'].apply(repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kind'] = df.kind.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['sim'] = 50*((1-df.loss)**500)\n",
    "df['sim'] = (1-df.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "df = df.dropna()[df.latency<0.02]\n",
    "sns.scatterplot(x=df.batch_size, y=df.latency, size=df.sim, marker='x', hue=df.kind)\n",
    "plt.ylim(0, 0.021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like `batch_size` of 16 with a jit model without autocast (AMP) is a safe bet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
